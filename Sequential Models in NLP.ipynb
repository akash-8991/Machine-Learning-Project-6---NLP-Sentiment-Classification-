{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXaFSkUu0fzm"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
    "\n",
    "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OudB5by50jlI"
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "### Dataset\n",
    "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
    "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
    "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "Command to import data\n",
    "- `from tensorflow.keras.datasets import imdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q34-Y3nRKXdO"
   },
   "source": [
    "### Import the data (2 Marks)\n",
    "- Use `imdb.load_data()` method\n",
    "- Get train and test set\n",
    "- Take 10000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxfwbrbuKbk2"
   },
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test)=imdb.load_data(path=\"imdb.pkl\",num_words=10000, skip_top=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DldivBO4LTbP"
   },
   "source": [
    "### Pad each sentence to be of same length (2 Marks)\n",
    "- Take maximum sequence length as 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E808XB4tLtic"
   },
   "outputs": [],
   "source": [
    "data = np.concatenate((x_train, x_test), axis=0)\n",
    "data1=pd.DataFrame(data)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "targets1=pd.DataFrame(targets)\n",
    "main_data = pd.concat([data1, targets1], axis=1)\n",
    "\n",
    "padded_inputs = pad_sequences(x_train, maxlen=300, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=300, value = 0.0) # 0.0 because it corresponds with <PAD>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFFCrybMSXz"
   },
   "source": [
    "### Print shape of features & labels (2 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOcyRtZfMYZd"
   },
   "source": [
    "Number of review, number of words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdMCUPr7RaCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in input label of train datset: (25000,)\n",
      "Rows in output label of train datset: (25000,)\n",
      "Rows in input label of test datset: (25000,)\n",
      "Rows in output label of test datset: (25000,)\n",
      "==========================================================================\n",
      "Number of reviews in dataset: 50000\n",
      "Review length of datset: 234.75892\n",
      "==========================================================================\n",
      "Number of reviews of train datset: 25000 & Review length of padded train datset: 300\n",
      "Number of reviews of test datset: 25000 & Review length of padded test datset: 300\n"
     ]
    }
   ],
   "source": [
    "print('Rows in input label of train datset:',x_train.shape)\n",
    "print('Rows in output label of train datset:',y_train.shape)\n",
    "print('Rows in input label of test datset:',x_test.shape)\n",
    "print('Rows in output label of test datset:',y_test.shape)\n",
    "print('==========================================================================')\n",
    "length = [len(i) for i in data]\n",
    "print(\"Number of reviews in dataset:\", data.shape[0])\n",
    "print(\"Review length of datset:\", np.mean(length))\n",
    "print('==========================================================================')\n",
    "print('Number of reviews of train datset:',padded_inputs.shape[0],'&','Review length of padded train datset:',padded_inputs.shape[1])\n",
    "print('Number of reviews of test datset:',padded_inputs_test.shape[0],'&','Review length of padded test datset:',padded_inputs_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cNk5sDvMr3j"
   },
   "source": [
    "Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Z00-mYgMoKv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "Categories: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(main_data.shape)\n",
    "print(\"Categories:\", np.unique(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdXPWuOmNEbh"
   },
   "source": [
    "### Print value of any one feature and it's label (2 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGLEdeFmNZfR"
   },
   "source": [
    "Feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKFyMa28zztL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# Obtain 1 texts\n",
    "j=0 #Printing value of feature and label of 0 index row\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[j]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_85Hqm0Nb1I"
   },
   "source": [
    "Label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FoehB5jNd1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Label:\", targets[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cof4LSxNxuv"
   },
   "source": [
    "### Decode the feature value to get original sentence (2 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_oiAyPZOkJD"
   },
   "source": [
    "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Clsk-yK8OtzD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 785,\n",
       " 189,\n",
       " 438,\n",
       " 47,\n",
       " 110,\n",
       " 142,\n",
       " 7,\n",
       " 6,\n",
       " 7475,\n",
       " 120,\n",
       " 4,\n",
       " 236,\n",
       " 378,\n",
       " 7,\n",
       " 153,\n",
       " 19,\n",
       " 87,\n",
       " 108,\n",
       " 141,\n",
       " 17,\n",
       " 1004,\n",
       " 5,\n",
       " 2,\n",
       " 883,\n",
       " 2,\n",
       " 23,\n",
       " 8,\n",
       " 4,\n",
       " 136,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 7475,\n",
       " 43,\n",
       " 1076,\n",
       " 21,\n",
       " 1407,\n",
       " 419,\n",
       " 5,\n",
       " 5202,\n",
       " 120,\n",
       " 91,\n",
       " 682,\n",
       " 189,\n",
       " 2818,\n",
       " 5,\n",
       " 9,\n",
       " 1348,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 118,\n",
       " 785,\n",
       " 189,\n",
       " 108,\n",
       " 126,\n",
       " 93,\n",
       " 2,\n",
       " 16,\n",
       " 540,\n",
       " 324,\n",
       " 23,\n",
       " 6,\n",
       " 364,\n",
       " 352,\n",
       " 21,\n",
       " 14,\n",
       " 9,\n",
       " 93,\n",
       " 56,\n",
       " 18,\n",
       " 11,\n",
       " 230,\n",
       " 53,\n",
       " 771,\n",
       " 74,\n",
       " 31,\n",
       " 34,\n",
       " 4,\n",
       " 2834,\n",
       " 7,\n",
       " 4,\n",
       " 22,\n",
       " 5,\n",
       " 14,\n",
       " 11,\n",
       " 471,\n",
       " 9,\n",
       " 2,\n",
       " 34,\n",
       " 4,\n",
       " 321,\n",
       " 487,\n",
       " 5,\n",
       " 116,\n",
       " 15,\n",
       " 6584,\n",
       " 4,\n",
       " 22,\n",
       " 9,\n",
       " 6,\n",
       " 2286,\n",
       " 4,\n",
       " 114,\n",
       " 2679,\n",
       " 23,\n",
       " 107,\n",
       " 293,\n",
       " 1008,\n",
       " 1172,\n",
       " 5,\n",
       " 328,\n",
       " 1236,\n",
       " 4,\n",
       " 1375,\n",
       " 109,\n",
       " 9,\n",
       " 6,\n",
       " 132,\n",
       " 773,\n",
       " 2,\n",
       " 1412,\n",
       " 8,\n",
       " 1172,\n",
       " 18,\n",
       " 7865,\n",
       " 29,\n",
       " 9,\n",
       " 276,\n",
       " 11,\n",
       " 6,\n",
       " 2768,\n",
       " 19,\n",
       " 289,\n",
       " 409,\n",
       " 4,\n",
       " 5341,\n",
       " 2140,\n",
       " 2,\n",
       " 648,\n",
       " 1430,\n",
       " 2,\n",
       " 8914,\n",
       " 5,\n",
       " 27,\n",
       " 3000,\n",
       " 1432,\n",
       " 7130,\n",
       " 103,\n",
       " 6,\n",
       " 346,\n",
       " 137,\n",
       " 11,\n",
       " 4,\n",
       " 2768,\n",
       " 295,\n",
       " 36,\n",
       " 7740,\n",
       " 725,\n",
       " 6,\n",
       " 3208,\n",
       " 273,\n",
       " 11,\n",
       " 4,\n",
       " 1513,\n",
       " 15,\n",
       " 1367,\n",
       " 35,\n",
       " 154,\n",
       " 2,\n",
       " 103,\n",
       " 2,\n",
       " 173,\n",
       " 7,\n",
       " 12,\n",
       " 36,\n",
       " 515,\n",
       " 3547,\n",
       " 94,\n",
       " 2547,\n",
       " 1722,\n",
       " 5,\n",
       " 3547,\n",
       " 36,\n",
       " 203,\n",
       " 30,\n",
       " 502,\n",
       " 8,\n",
       " 361,\n",
       " 12,\n",
       " 8,\n",
       " 989,\n",
       " 143,\n",
       " 4,\n",
       " 1172,\n",
       " 3404,\n",
       " 10,\n",
       " 10,\n",
       " 328,\n",
       " 1236,\n",
       " 9,\n",
       " 6,\n",
       " 55,\n",
       " 221,\n",
       " 2989,\n",
       " 5,\n",
       " 146,\n",
       " 165,\n",
       " 179,\n",
       " 770,\n",
       " 15,\n",
       " 50,\n",
       " 713,\n",
       " 53,\n",
       " 108,\n",
       " 448,\n",
       " 23,\n",
       " 12,\n",
       " 17,\n",
       " 225,\n",
       " 38,\n",
       " 76,\n",
       " 4397,\n",
       " 18,\n",
       " 183,\n",
       " 8,\n",
       " 81,\n",
       " 19,\n",
       " 12,\n",
       " 45,\n",
       " 1257,\n",
       " 8,\n",
       " 135,\n",
       " 15,\n",
       " 2,\n",
       " 166,\n",
       " 4,\n",
       " 118,\n",
       " 7,\n",
       " 45,\n",
       " 2,\n",
       " 17,\n",
       " 466,\n",
       " 45,\n",
       " 2,\n",
       " 4,\n",
       " 22,\n",
       " 115,\n",
       " 165,\n",
       " 764,\n",
       " 6075,\n",
       " 5,\n",
       " 1030,\n",
       " 8,\n",
       " 2973,\n",
       " 73,\n",
       " 469,\n",
       " 167,\n",
       " 2127,\n",
       " 2,\n",
       " 1568,\n",
       " 6,\n",
       " 87,\n",
       " 841,\n",
       " 18,\n",
       " 4,\n",
       " 22,\n",
       " 4,\n",
       " 192,\n",
       " 15,\n",
       " 91,\n",
       " 7,\n",
       " 12,\n",
       " 304,\n",
       " 273,\n",
       " 1004,\n",
       " 4,\n",
       " 1375,\n",
       " 1172,\n",
       " 2768,\n",
       " 2,\n",
       " 15,\n",
       " 4,\n",
       " 22,\n",
       " 764,\n",
       " 55,\n",
       " 5773,\n",
       " 5,\n",
       " 14,\n",
       " 4233,\n",
       " 7444,\n",
       " 4,\n",
       " 1375,\n",
       " 326,\n",
       " 7,\n",
       " 4,\n",
       " 4760,\n",
       " 1786,\n",
       " 8,\n",
       " 361,\n",
       " 1236,\n",
       " 8,\n",
       " 989,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2768,\n",
       " 45,\n",
       " 55,\n",
       " 776,\n",
       " 8,\n",
       " 79,\n",
       " 496,\n",
       " 98,\n",
       " 45,\n",
       " 400,\n",
       " 301,\n",
       " 15,\n",
       " 4,\n",
       " 1859,\n",
       " 9,\n",
       " 4,\n",
       " 155,\n",
       " 15,\n",
       " 66,\n",
       " 2,\n",
       " 84,\n",
       " 5,\n",
       " 14,\n",
       " 22,\n",
       " 1534,\n",
       " 15,\n",
       " 17,\n",
       " 4,\n",
       " 167,\n",
       " 2,\n",
       " 15,\n",
       " 75,\n",
       " 70,\n",
       " 115,\n",
       " 66,\n",
       " 30,\n",
       " 252,\n",
       " 7,\n",
       " 618,\n",
       " 51,\n",
       " 9,\n",
       " 2161,\n",
       " 4,\n",
       " 3130,\n",
       " 5,\n",
       " 14,\n",
       " 1525,\n",
       " 8,\n",
       " 6584,\n",
       " 15,\n",
       " 2,\n",
       " 165,\n",
       " 127,\n",
       " 1921,\n",
       " 8,\n",
       " 30,\n",
       " 179,\n",
       " 2532,\n",
       " 4,\n",
       " 22,\n",
       " 9,\n",
       " 906,\n",
       " 18,\n",
       " 6,\n",
       " 176,\n",
       " 7,\n",
       " 1007,\n",
       " 1005,\n",
       " 4,\n",
       " 1375,\n",
       " 114,\n",
       " 4,\n",
       " 105,\n",
       " 26,\n",
       " 32,\n",
       " 55,\n",
       " 221,\n",
       " 11,\n",
       " 68,\n",
       " 205,\n",
       " 96,\n",
       " 5,\n",
       " 4,\n",
       " 192,\n",
       " 15,\n",
       " 4,\n",
       " 274,\n",
       " 410,\n",
       " 220,\n",
       " 304,\n",
       " 23,\n",
       " 94,\n",
       " 205,\n",
       " 109,\n",
       " 9,\n",
       " 55,\n",
       " 73,\n",
       " 224,\n",
       " 259,\n",
       " 3786,\n",
       " 15,\n",
       " 4,\n",
       " 22,\n",
       " 528,\n",
       " 1645,\n",
       " 34,\n",
       " 4,\n",
       " 130,\n",
       " 528,\n",
       " 30,\n",
       " 685,\n",
       " 345,\n",
       " 17,\n",
       " 4,\n",
       " 277,\n",
       " 199,\n",
       " 166,\n",
       " 281,\n",
       " 5,\n",
       " 1030,\n",
       " 8,\n",
       " 30,\n",
       " 179,\n",
       " 4442,\n",
       " 444,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 371,\n",
       " 87,\n",
       " 189,\n",
       " 22,\n",
       " 5,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 118,\n",
       " 7,\n",
       " 4,\n",
       " 2068,\n",
       " 545,\n",
       " 1178,\n",
       " 829]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=10 #Decoding value of feature\n",
    "data[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgOD5S2Uuvd"
   },
   "source": [
    "Now use the dictionary to get the original words from the encodings, for a particular sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ504QDORwxj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# french horror cinema has seen something of a revival over the last couple of years with great films such as inside and # romance # on to the scene # # the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made # was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is # by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named # sent to prison for fraud he is put in a cell with three others the quietly insane # body building # marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old # after # part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that # makes the best of it's # as despite it's # the film never actually feels restrained and manages to flow well throughout director eric # provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell # that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really # people and this film proves that as the director # that we can never really be sure of exactly what is round the corner and this helps to ensure that # actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall # is a truly great horror film and one of the best of the decade highly recommended viewing\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[k]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLGABrJoVZe6"
   },
   "source": [
    "Get the sentiment for the above sentence\n",
    "- positive (1)\n",
    "- negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDyQGJT0Ve-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Label:\", targets[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmCjr8miXIWB"
   },
   "source": [
    "### Define model (10 Marks)\n",
    "- Define a Sequential Model\n",
    "- Add Embedding layer\n",
    "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
    "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
    "  - Size of the vocabulary will be 10000\n",
    "  - Give dimension of the dense embedding as 100\n",
    "  - Length of input sequences should be 300\n",
    "- Add LSTM layer\n",
    "  - Pass value in `return_sequences` as True\n",
    "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
    "- Add Flatten layer\n",
    "- Add Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np5GxT1caFEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Prepare data for Modelling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "\n",
    "# Model configuration\n",
    "max_sequence_length = 300\n",
    "num_distinct_words = 10000\n",
    "embedding_output_dims = 15\n",
    "loss_function = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "additional_metrics = ['accuracy']\n",
    "number_of_epochs = 100\n",
    "verbosity_mode = True\n",
    "validation_split = 0.20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hc4bknOobDby"
   },
   "source": [
    "### Compile the model (2 Marks)\n",
    "- Use Optimizer as Adam\n",
    "- Use Binary Crossentropy as loss\n",
    "- Use Accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw4RJ0CQbwFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 15)           150000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 15)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 300, 32)           992       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4801      \n",
      "=================================================================\n",
      "Total params: 155,793\n",
      "Trainable params: 155,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 35s 2ms/sample - loss: 0.6636 - acc: 0.5710 - val_loss: 0.4811 - val_acc: 0.7954\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 20s 1ms/sample - loss: 0.3487 - acc: 0.8514 - val_loss: 0.3093 - val_acc: 0.8778\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 21s 1ms/sample - loss: 0.2487 - acc: 0.8984 - val_loss: 0.2758 - val_acc: 0.8870\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 20s 985us/sample - loss: 0.2092 - acc: 0.9172 - val_loss: 0.2791 - val_acc: 0.8844\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 23s 1ms/sample - loss: 0.1839 - acc: 0.9276 - val_loss: 0.2834 - val_acc: 0.8862\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 20s 984us/sample - loss: 0.1675 - acc: 0.9340 - val_loss: 0.2978 - val_acc: 0.8760\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 18s 881us/sample - loss: 0.1511 - acc: 0.9414 - val_loss: 0.3028 - val_acc: 0.8812\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 18s 875us/sample - loss: 0.1416 - acc: 0.9432 - val_loss: 0.3188 - val_acc: 0.8832\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 18s 890us/sample - loss: 0.1366 - acc: 0.9471 - val_loss: 0.3396 - val_acc: 0.8748\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 16s 788us/sample - loss: 0.1274 - acc: 0.9503 - val_loss: 0.3379 - val_acc: 0.8782\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 16s 809us/sample - loss: 0.1217 - acc: 0.9513 - val_loss: 0.3518 - val_acc: 0.8794\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 16s 800us/sample - loss: 0.1219 - acc: 0.9519 - val_loss: 0.3770 - val_acc: 0.8702\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 16s 795us/sample - loss: 0.1128 - acc: 0.9565 - val_loss: 0.3727 - val_acc: 0.8792\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 16s 802us/sample - loss: 0.1102 - acc: 0.9549 - val_loss: 0.3875 - val_acc: 0.8752\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 17s 843us/sample - loss: 0.1047 - acc: 0.9582 - val_loss: 0.4033 - val_acc: 0.8734\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 17s 846us/sample - loss: 0.1008 - acc: 0.9621 - val_loss: 0.4091 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 22s 1ms/sample - loss: 0.1033 - acc: 0.9597 - val_loss: 0.4077 - val_acc: 0.8738\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 16s 803us/sample - loss: 0.1006 - acc: 0.9599 - val_loss: 0.4707 - val_acc: 0.8580\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 16s 794us/sample - loss: 0.0963 - acc: 0.9629 - val_loss: 0.4627 - val_acc: 0.8656\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 16s 781us/sample - loss: 0.0937 - acc: 0.9639 - val_loss: 0.4447 - val_acc: 0.8700\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 16s 779us/sample - loss: 0.0932 - acc: 0.9640 - val_loss: 0.4451 - val_acc: 0.8708\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 16s 805us/sample - loss: 0.0875 - acc: 0.9660 - val_loss: 0.4545 - val_acc: 0.8714\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 17s 828us/sample - loss: 0.0929 - acc: 0.9647 - val_loss: 0.4557 - val_acc: 0.8712\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 16s 794us/sample - loss: 0.0906 - acc: 0.9658 - val_loss: 0.4478 - val_acc: 0.8738\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 16s 791us/sample - loss: 0.0854 - acc: 0.9679 - val_loss: 0.4514 - val_acc: 0.8740\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 17s 843us/sample - loss: 0.0839 - acc: 0.9689 - val_loss: 0.4610 - val_acc: 0.8732\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 18s 920us/sample - loss: 0.0864 - acc: 0.9684 - val_loss: 0.4720 - val_acc: 0.8742\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 17s 846us/sample - loss: 0.0874 - acc: 0.9671 - val_loss: 0.4809 - val_acc: 0.8716\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 17s 865us/sample - loss: 0.0823 - acc: 0.9686 - val_loss: 0.4852 - val_acc: 0.8684\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 17s 854us/sample - loss: 0.0848 - acc: 0.9674 - val_loss: 0.4704 - val_acc: 0.8720\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 17s 862us/sample - loss: 0.0803 - acc: 0.9697 - val_loss: 0.4852 - val_acc: 0.8718\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 17s 869us/sample - loss: 0.0827 - acc: 0.9678 - val_loss: 0.4859 - val_acc: 0.8696\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 18s 888us/sample - loss: 0.0818 - acc: 0.9682 - val_loss: 0.4961 - val_acc: 0.8672\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 17s 872us/sample - loss: 0.0792 - acc: 0.9690 - val_loss: 0.5008 - val_acc: 0.8676\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 17s 843us/sample - loss: 0.0746 - acc: 0.9693 - val_loss: 0.5125 - val_acc: 0.8676\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 18s 901us/sample - loss: 0.0786 - acc: 0.9707 - val_loss: 0.5160 - val_acc: 0.8700\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 18s 905us/sample - loss: 0.0786 - acc: 0.9702 - val_loss: 0.5088 - val_acc: 0.8716\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 17s 870us/sample - loss: 0.0787 - acc: 0.9711 - val_loss: 0.5040 - val_acc: 0.8690\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 18s 880us/sample - loss: 0.0782 - acc: 0.9700 - val_loss: 0.5139 - val_acc: 0.8688loss: 0.\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 18s 889us/sample - loss: 0.0726 - acc: 0.9730 - val_loss: 0.5155 - val_acc: 0.8676\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 18s 898us/sample - loss: 0.0789 - acc: 0.9697 - val_loss: 0.5162 - val_acc: 0.8686\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 17s 868us/sample - loss: 0.0757 - acc: 0.9707 - val_loss: 0.5323 - val_acc: 0.8658\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 18s 888us/sample - loss: 0.0772 - acc: 0.9717 - val_loss: 0.5193 - val_acc: 0.8680\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 17s 874us/sample - loss: 0.0758 - acc: 0.9713 - val_loss: 0.5484 - val_acc: 0.8654\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 18s 888us/sample - loss: 0.0724 - acc: 0.9718 - val_loss: 0.5295 - val_acc: 0.8694\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 16s 821us/sample - loss: 0.0641 - acc: 0.9747 - val_loss: 0.5346 - val_acc: 0.8674\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 16s 821us/sample - loss: 0.0691 - acc: 0.9739 - val_loss: 0.5412 - val_acc: 0.8678\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 17s 858us/sample - loss: 0.0655 - acc: 0.9761 - val_loss: 0.5760 - val_acc: 0.8656\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 17s 834us/sample - loss: 0.0698 - acc: 0.9746 - val_loss: 0.5423 - val_acc: 0.8690\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 17s 846us/sample - loss: 0.0674 - acc: 0.9749 - val_loss: 0.5711 - val_acc: 0.8664\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 17s 845us/sample - loss: 0.0697 - acc: 0.9737 - val_loss: 0.5562 - val_acc: 0.8704\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 17s 842us/sample - loss: 0.0715 - acc: 0.9744 - val_loss: 0.5507 - val_acc: 0.8682\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 17s 835us/sample - loss: 0.0743 - acc: 0.9719 - val_loss: 0.5461 - val_acc: 0.8682\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 17s 872us/sample - loss: 0.0673 - acc: 0.9758 - val_loss: 0.5570 - val_acc: 0.8704\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 17s 832us/sample - loss: 0.0632 - acc: 0.9761 - val_loss: 0.5634 - val_acc: 0.8702\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 17s 848us/sample - loss: 0.0713 - acc: 0.9740 - val_loss: 0.5559 - val_acc: 0.8706\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 17s 849us/sample - loss: 0.0647 - acc: 0.9759 - val_loss: 0.5527 - val_acc: 0.8698\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 17s 858us/sample - loss: 0.0632 - acc: 0.9756 - val_loss: 0.5580 - val_acc: 0.8694\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 17s 845us/sample - loss: 0.0685 - acc: 0.9750 - val_loss: 0.5657 - val_acc: 0.8720\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 17s 845us/sample - loss: 0.0683 - acc: 0.9747 - val_loss: 0.5535 - val_acc: 0.8700\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 17s 849us/sample - loss: 0.0670 - acc: 0.9758 - val_loss: 0.5690 - val_acc: 0.8684\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 17s 855us/sample - loss: 0.0658 - acc: 0.9746 - val_loss: 0.5733 - val_acc: 0.8682\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 18s 876us/sample - loss: 0.0667 - acc: 0.9750 - val_loss: 0.5637 - val_acc: 0.8684\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 18s 876us/sample - loss: 0.0650 - acc: 0.9753 - val_loss: 0.5690 - val_acc: 0.8684\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 18s 879us/sample - loss: 0.0667 - acc: 0.9743 - val_loss: 0.5741 - val_acc: 0.8672\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 18s 885us/sample - loss: 0.0661 - acc: 0.9751 - val_loss: 0.5720 - val_acc: 0.8672\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 17s 849us/sample - loss: 0.0653 - acc: 0.9749 - val_loss: 0.5701 - val_acc: 0.8682\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 17s 864us/sample - loss: 0.0662 - acc: 0.9756 - val_loss: 0.5639 - val_acc: 0.8660\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 17s 860us/sample - loss: 0.0627 - acc: 0.9765 - val_loss: 0.5743 - val_acc: 0.8678\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 19s 960us/sample - loss: 0.0676 - acc: 0.9746 - val_loss: 0.5691 - val_acc: 0.8656\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 19s 942us/sample - loss: 0.0604 - acc: 0.9764 - val_loss: 0.6000 - val_acc: 0.8634\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 18s 905us/sample - loss: 0.0686 - acc: 0.9743 - val_loss: 0.5833 - val_acc: 0.8656\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 17s 875us/sample - loss: 0.0627 - acc: 0.9776 - val_loss: 0.5877 - val_acc: 0.8646\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 18s 889us/sample - loss: 0.0641 - acc: 0.9767 - val_loss: 0.5849 - val_acc: 0.8644\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 17s 870us/sample - loss: 0.0608 - acc: 0.9786 - val_loss: 0.5998 - val_acc: 0.8658\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 17s 849us/sample - loss: 0.0641 - acc: 0.9771 - val_loss: 0.5968 - val_acc: 0.8648\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 17s 862us/sample - loss: 0.0609 - acc: 0.9775 - val_loss: 0.6030 - val_acc: 0.8674\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 17s 874us/sample - loss: 0.0603 - acc: 0.9776 - val_loss: 0.6392 - val_acc: 0.8640\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 18s 883us/sample - loss: 0.0667 - acc: 0.9751 - val_loss: 0.6071 - val_acc: 0.8654\n",
      "Epoch 80/100\n",
      "20000/20000 [==============================] - 18s 886us/sample - loss: 0.0615 - acc: 0.9768 - val_loss: 0.6422 - val_acc: 0.8626\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 19s 932us/sample - loss: 0.0621 - acc: 0.9766 - val_loss: 0.6143 - val_acc: 0.8654\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 18s 916us/sample - loss: 0.0603 - acc: 0.9774 - val_loss: 0.6080 - val_acc: 0.8652\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 17s 867us/sample - loss: 0.0658 - acc: 0.9768 - val_loss: 0.5982 - val_acc: 0.8668\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 17s 873us/sample - loss: 0.0584 - acc: 0.9783 - val_loss: 0.6056 - val_acc: 0.8676\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 18s 894us/sample - loss: 0.0667 - acc: 0.9753 - val_loss: 0.6038 - val_acc: 0.8668\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 18s 905us/sample - loss: 0.0581 - acc: 0.9782 - val_loss: 0.6240 - val_acc: 0.8674\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 18s 899us/sample - loss: 0.0588 - acc: 0.9788 - val_loss: 0.6134 - val_acc: 0.8678\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 18s 911us/sample - loss: 0.0605 - acc: 0.9772 - val_loss: 0.6166 - val_acc: 0.8684\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 17s 866us/sample - loss: 0.0642 - acc: 0.9761 - val_loss: 0.6169 - val_acc: 0.8638\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 18s 876us/sample - loss: 0.0626 - acc: 0.9763 - val_loss: 0.6062 - val_acc: 0.8662\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 18s 890us/sample - loss: 0.0591 - acc: 0.9782 - val_loss: 0.6242 - val_acc: 0.8648\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 17s 867us/sample - loss: 0.0609 - acc: 0.9772 - val_loss: 0.6127 - val_acc: 0.8660\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 17s 874us/sample - loss: 0.0596 - acc: 0.9779 - val_loss: 0.6148 - val_acc: 0.8670\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 17s 865us/sample - loss: 0.0579 - acc: 0.9783 - val_loss: 0.6297 - val_acc: 0.8642\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 17s 854us/sample - loss: 0.0591 - acc: 0.9778 - val_loss: 0.6103 - val_acc: 0.8680\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 17s 870us/sample - loss: 0.0563 - acc: 0.9793 - val_loss: 0.6081 - val_acc: 0.8654\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 18s 883us/sample - loss: 0.0553 - acc: 0.9804 - val_loss: 0.6102 - val_acc: 0.8648\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 18s 893us/sample - loss: 0.0647 - acc: 0.9759 - val_loss: 0.6023 - val_acc: 0.8680\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 19s 967us/sample - loss: 0.0570 - acc: 0.9793 - val_loss: 0.6191 - val_acc: 0.8678\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 18s 914us/sample - loss: 0.0563 - acc: 0.9784 - val_loss: 0.6288 - val_acc: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_inputs, y_train, epochs=number_of_epochs, verbose=verbosity_mode, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEzwazqbz3T"
   },
   "source": [
    "### Print model summary (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hx1yxwlb2Ue"
   },
   "outputs": [],
   "source": [
    "# Give a summary\n",
    "# Ran in above cell\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmkolKP4b-U6"
   },
   "source": [
    "### Fit the model (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRg3KFXLcAkk"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Ran in above cell\n",
    "history = model.fit(padded_inputs, y_train, epochs=number_of_epochs, verbose=verbosity_mode, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwLl54MXnkEA"
   },
   "source": [
    "### Evaluate model (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUqY-bD8RaDR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results - Loss: 0.6975063483738899 - Accuracy: 84.74400043487549%\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2amr1tJn9Jz"
   },
   "source": [
    "### Predict on one sample (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wl4idfWR_A8E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Ouput: [0.] & Actual Output: 0\n",
      "Predicted Ouput: [1.] & Actual Output: 1\n",
      "Predicted Ouput: [0.] & Actual Output: 1\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for first 3 samples\n",
    "model_predict=model.predict(padded_inputs_test).round()\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    print(\"Predicted Ouput:\",model_predict[i],\"&\",\"Actual Output:\",y_test[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
